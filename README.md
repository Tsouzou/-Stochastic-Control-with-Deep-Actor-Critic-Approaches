# Stochastic Control with Deep Actor-Critic Approaches

This project explores a range of methods for controlling linear-quadratic systems subject to stochastic disturbances. It begins with deriving and implementing a rigorous Riccati-based approach for the classical, or “strict,” linear-quadratic regulator (LQR). The key steps involve solving a continuous-time Riccati ordinary differential equation (ODE) with final boundary conditions and then using its solution to construct both the value function and the optimal feedback law. In parallel, time-discretized stochastic simulations verify the theoretical predictions by numerically integrating the controlled stochastic differential equation through an Euler-Maruyama scheme.  

Building on the strict LQR problem, an entropically regularized or “soft” LQR variant introduces a penalty on concentrated controls, which modifies the effective cost and changes the Riccati equation. The project compares simulation results for strict and soft controllers under the same stochastic inputs and shows that the latter yields smoother control trajectories, while the former can occasionally produce more sudden corrections. Both methods ultimately drive the system state toward the origin, confirming their fundamental similarity.  

Subsequent portions of the work shift toward a deep reinforcement learning perspective. A fixed “best-known” policy is used initially so that a neural network “critic” can learn the associated cost-to-go value function from Monte Carlo returns. Each episode entails sampling an initial state, generating a trajectory under the fixed policy, and computing the total observed cost. The critic then fits these returns via mean-squared-error minimization. This approach reveals an in-depth mastery of function approximation and backpropagation, with occasional spikes in the critic’s loss tracing to sampling randomness.  

Further progress appears in a policy gradient implementations where the project removes dependence on a given policy by training an “actor” network that outputs the mean of a Gaussian distribution over actions. The advantage function is computed in single-step increments, and gradient steps adjust the actor’s parameters to reduce cost. Here, fluctuations in the actor’s loss reflect the stochastic nature of direct policy sampling, yet simulation-based rollouts sometimes reach lower costs than the baseline—an indication of how random exploration can yield slight improvements in state trajectories.  

Finally, a combined actor-critic framework jointly learns both the policy and the value function without relying on any precomputed Riccati solution. This requires careful handling of temporal differences, where the critic estimates next-step discounted costs and the actor adjusts its parameters in response. Even though the training losses can rise over time as the system explores more complex configurations, it demonstrates that a policy and value function can be co-learned effectively from scratch. Consistent improvements in trajectory costs speak to the success of this integrated approach.  

The depth of mathematics on display ranges from applying linear algebra to solve matrix Riccati ODEs, to discretizing stochastic processes, to understanding how entropic regularization modifies the cost functional, to designing and tuning neural networks for policy gradient. The coding effort is equally impressive. The project separates each component into clean and testable Python modules, carefully orchestrating environment instantiation, ODE solvers, simulation loops, and neural network training in a modular but integrated fashion. The resulting Python code exhibits not only a thorough command of deep learning libraries but also an attention to numerical detail, computational efficiency, and clear structuring of abstractions.  

By mixing classical optimal control with modern deep reinforcement learning, the project demonstrates an ability to unify multiple fields into a cohesive toolkit for stochastic control. Mathematical sophistication is evident in the derivations and justifications behind Riccati theory, stochastic calculus, and policy gradient objectives, while coding skill is clear from the smooth execution of each component in a unified workflow. The collection of numerical experiments, plots, and discussions underscores the strong command of both the theoretical underpinnings and the practical implementation details of advanced control algorithms.
